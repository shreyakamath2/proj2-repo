{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54d874d-a65a-478f-b2ab-8a737cfd2b33",
   "metadata": {},
   "source": [
    "# Project #2: Web Scraping Data Analysis & Visualization\n",
    "## By: Shreya Kamath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41617a-9e2c-4706-a846-d7d24c727654",
   "metadata": {},
   "source": [
    "### In this project, I will be analyzing Wikipedia Data surrounding a central theme: movies. I plan on analyzing data regarding awards, notable actors and actresses, and film earnings.\n",
    "### Some of the questions I am trying to answer include:\n",
    "### 1) Which decade had the most top-earning movies?\n",
    "### 2) Do the highest-grossing movies have a better likelihood of being nominated for the Academy Award for Best Picture?\n",
    "### 3) Do the highest-paid actors and actresses get nominated for more Academy Awards for Best Actor/Actress?\n",
    "### 4) Do more expensive films make more money at the box office?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52c601-9588-482f-b7c1-bae0636ce014",
   "metadata": {},
   "source": [
    "# Part 1: Package Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456bac51-131d-4c71-a251-eb2e72e9f374",
   "metadata": {},
   "source": [
    "### In this section, I will be importing all the necessary packages needed for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c21423f-2b5d-40c4-8407-5562ed3c7ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a18da0-e07b-48c6-b30c-bce9479e19ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frames\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1788b-4e2e-43e0-afdf-b6a8245aeb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c42e4-c747-4e8f-af25-231941a0679b",
   "metadata": {},
   "source": [
    "# Part 2: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72ce44-e56f-4504-b5be-fbc965d615ea",
   "metadata": {},
   "source": [
    "### Note: In this section, I followed this video tutorial: https://www.youtube.com/watch?v=8dTpNajxaH0 to help me scrape my Wikipedia Pages. Although I ended up having to make some of my own tweaks during the scraping process, this tutorial provided a great starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e066625-04b7-4cf9-99f6-306ffb02a8e7",
   "metadata": {},
   "source": [
    "## Webpage 1: Highest Grossing Films"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee623c43-19e5-49c5-bad3-163007ee162d",
   "metadata": {},
   "source": [
    "### Step 1: Setting up a web scraping pipeline to collect all the data from the Wikitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7c909-926e-4aca-a54b-3eea627dfbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the text from the webpage and placing it in a soup object\n",
    "url1 = 'https://en.wikipedia.org/wiki/List_of_highest-grossing_films_in_the_United_States_and_Canada'\n",
    "page1 = requests.get(url1)\n",
    "soup1 = BeautifulSoup(page1.text, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d688b-aa05-4fa6-ac31-4a6159ed78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d14750-2199-4b0f-9372-32204db2a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the exact table on the webpage I want to get data from\n",
    "table1 = soup1.find_all('table', class_ = 'wikitable sortable plainrowheaders')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f53b8-1603-4582-8bc6-cb21d32a3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e1080-28b8-4e4c-96fe-6e89b52bd773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all the titles for the table in the <th> tags\n",
    "titles1 = table1.find_all('th')\n",
    "table1_titles = [title.text.strip() for title in titles1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce1214-ee7e-4653-9ac5-7eaab989d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2d3c4d-0a3a-47f2-9542-ae8c461025c0",
   "metadata": {},
   "source": [
    "### Step 2: Creating a Pandas dataframe and placing the scraped data into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34be1f6-0d66-47ee-8d45-06fb4c978ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame with the table titles as column names\n",
    "highestGrossing = pd.DataFrame(columns = table1_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1aa498-fb7c-410b-958a-43da9b59c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "highestGrossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0046e4-ba46-4d6a-9c52-1049e54d1550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the data to fill the columns within the <tr> tags\n",
    "tbl1_col_data = table1.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746cfe20-2bb4-4007-9ac5-d7c3e4511b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tbl1_col_data[1:]: # Start at the second row to ignore the header tags\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    length = len(highestGrossing) # Get the number of columns in the dataframe\n",
    "    highestGrossing.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e40ca-1454-4697-bd68-235352b0f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of highest grossing films adjusted for inflation\n",
    "highestGrossing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034673e0-90bb-4faa-beed-884fe462736e",
   "metadata": {},
   "source": [
    "## Webpage 2: Most Expensive Films"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c914dac8-e998-4f40-9844-c9c0e262a49b",
   "metadata": {},
   "source": [
    "### Step 1: Setting up a web scraping pipeline to collect all the data from the Wikitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e4a77-50d9-4e91-b7e1-3e3fc8958fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the text from the webpage and placing it in a soup object\n",
    "url2 = 'https://en.wikipedia.org/wiki/List_of_most_expensive_films'\n",
    "page2 = requests.get(url2)\n",
    "soup2 = BeautifulSoup(page2.text, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e17276a-143e-4f57-b0e5-b0730c2b2186",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430da56-3841-474a-881c-fc42456689f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the exact table on the webpage I want to get data from\n",
    "table2 = soup2.find_all('table', class_ = 'wikitable sortable plainrowheaders')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dfab82-8e41-43c0-9237-d39b1036d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f578b12-8c08-46fb-aea4-a595dab8561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all the titles for the table in the <th> tags\n",
    "titles2 = table2.find_all('th')\n",
    "table2_titles = [title.text.strip() for title in titles2]\n",
    "table2_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9d662-9cb1-4f48-ad65-b1741d611ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the list to have two seperate lists: one for actual table headers, and one for movie titles\n",
    "table_headers = table2_titles[:5]\n",
    "table_headers\n",
    "movie_titles = table2_titles[5:]\n",
    "movie_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b920f0e-71cd-4ad3-9739-60a83fe76437",
   "metadata": {},
   "source": [
    "#### Note: This table had placed the movie titles within header tags for some reason, so I had to split the list like this in order to still be able to keep the movie titles to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c91e70-e57b-4bf4-9a9a-c8fe82d9c70a",
   "metadata": {},
   "source": [
    "### Step 2: Creating a Pandas dataframe and placing the scraped data into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee1ded-4186-4a40-b7d8-e52c6cee460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame with the table titles as column names\n",
    "mostExpensive = pd.DataFrame(columns = table_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30f038-9f37-408e-9393-5a21f4795afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns\n",
    "mostExpensive.drop('Rank', axis=1, inplace=True)\n",
    "mostExpensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bff28a-10d6-4e6f-b4ce-6583369b22a7",
   "metadata": {},
   "source": [
    "#### Note: Because I did not require this column, and there were issues being caused by the movie titles being in header tags, I decided to remove this column beforehand to make the process of putting the data into the dataframe easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2d877-53bb-45e0-9a6d-b3bd17b6f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the data to fill the columns within the <tr> tags\n",
    "tbl2_col_data = table2.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff2bf2-c641-4c52-b087-98bb0852289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0 # Create an index to loop through the movies list to be able to add it to the df\n",
    "for row in tbl2_col_data[1:]: # Start at the second row to ignore the header row\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    individual_row_data.insert(1, movie_titles[index]) # Adding the movie title that was initially cast in a <th> tag to the row data\n",
    "    individual_row_data = individual_row_data[1:] # Slicing at this index to remove the rank data, as that column was dropped\n",
    "    if len(individual_row_data) != 4: # To deal with movies that having missing data due to the odd structure of the Wikitable\n",
    "        individual_row_data.insert(1, 'XXXX') # Put a placeholder in the year column, because it will eventually be removed\n",
    "    index += 1 \n",
    "    length = len(mostExpensive) # Get the number of columns in the dataframe\n",
    "    mostExpensive.loc[length] = individual_row_data  # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c383e4-48ee-4bf5-a7cd-27b34d2f65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mostExpensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e98fb7f-fcb9-40aa-9235-629485342633",
   "metadata": {},
   "source": [
    "### Step 3: Performing additional cleaning operations on the dataframe to make it useful for answering my question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d3874-de79-45f7-a3ac-06cea72cc11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are unecessary for my data analysis\n",
    "mostExpensive.drop('Year', axis=1, inplace=True)\n",
    "mostExpensive.drop('Refs and notes', axis=1, inplace=True)\n",
    "mostExpensive\n",
    "# List of most expensive films to create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab518d-dcdc-48fd-97d1-bdd0534a5936",
   "metadata": {},
   "source": [
    "#### Note: I dropped these columns because they weren't necessary for my data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e6eab3-b6d2-40b1-ad24-46ae3b8e2041",
   "metadata": {},
   "source": [
    "## Webpage 3: Nominees for Academy Award for Best Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc15efd-e675-4926-a8d1-b2c4d62599d4",
   "metadata": {},
   "source": [
    "### Step 1: Setting up a web scraping pipeline to collect all the data from the Wikitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae6721-5064-44d8-88ad-6d3fe0079e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the text from the webpage and placing it in a soup object\n",
    "url3 = 'https://en.wikipedia.org/wiki/Academy_Award_for_Best_Picture'\n",
    "page3 = requests.get(url3)\n",
    "soup3 = BeautifulSoup(page3.text, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adffe47-b4c0-4432-ac16-0eee425f1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5da344-4be4-4a30-a2cf-7e54ce0958d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the exact table on the webpage I want to get data from\n",
    "table3 = soup3.find_all('table', class_ = 'wikitable sortable sticky-header')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72341c04-224f-43cf-9fd8-09d226fcde15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all the titles for the table in the <th> tags\n",
    "titles3 = table3.find_all('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a8271-df0d-4c15-b503-4f1688f9776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table3_titles = [title.text.strip() for title in titles3]\n",
    "table3_titles = table3_titles[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca5314-2adc-400f-a300-e95208fa16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "table3_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9bf52-9a5e-4794-9fcd-0d4347f086b7",
   "metadata": {},
   "source": [
    "### Step 2: Creating a Pandas dataframe and placing the scraped data into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02857fe8-23a6-467a-af9b-a161e7ed6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame with the table titles as column names\n",
    "bestPicture = pd.DataFrame(columns = table3_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030a0ca-f831-442f-a6d1-47a9b0e76848",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestPicture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedd5ef-7de4-4936-884c-bed3bf9e5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Unecessary columns\n",
    "bestPicture.drop('Year of Film Release', axis=1, inplace=True)\n",
    "bestPicture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ce7a4-9b41-4fdf-a03d-62b684bd2823",
   "metadata": {},
   "source": [
    "#### Note: The year column was unecessary for my analysis, so I got rid of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323baf9-ece1-46fb-8f03-d2dcb440aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "noms_tbl1 = soup3.find_all('table', class_ = 'wikitable sortable sticky-header')[0] # Finding the exact table on the webpage I want to get data from\n",
    "noms_col_data1 = noms_tbl1.find_all('tr') # Finding the data to fill the columns within the <tr> tags\n",
    "for row in noms_col_data1:\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    length = len(bestPicture) # Get the number of columns in the dataframe\n",
    "    if not individual_row_data: # If the list of data is empty - indicates header row\n",
    "        continue # Don't add it to the df, just move onto the next row in the Wikitable\n",
    "    else:\n",
    "        bestPicture.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ff872-3c37-45a9-9895-198824c0576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "noms_tbl2 = soup3.find_all('table', class_ = 'wikitable sortable sticky-header')[1] # Finding the exact table on the webpage I want to get data from\n",
    "noms_col_data2 = noms_tbl2.find_all('tr') # Finding the data to fill the columns within the <tr> tags\n",
    "for row in noms_col_data2:\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    length = len(bestPicture) # Get the number of columns in the dataframe\n",
    "    if not individual_row_data: # If the list of data is empty - indicates header row\n",
    "        continue # Don't add it to the df, just move onto the next row in the Wikitable\n",
    "    else:  \n",
    "        bestPicture.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e247d70-81e9-4cfe-a57d-b824e31c1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "noms_tbl3 = soup3.find_all('table', class_ = 'wikitable sortable sticky-header')[2] # Finding the exact table on the webpage I want to get data from\n",
    "noms_col_data3 = noms_tbl3.find_all('tr') # Finding the data to fill the columns within the <tr> tags\n",
    "for row in noms_col_data3:\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    length = len(bestPicture) # Get the number of columns in the dataframe\n",
    "    if not individual_row_data: # If the list of data is empty - indicates header row\n",
    "        continue # Don't add it to the df, just move onto the next row in the Wikitable\n",
    "    else:\n",
    "        bestPicture.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4a7b5-c18f-490d-916b-34cb024c0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "noms_tbl4 = soup3.find_all('table', class_ = 'wikitable sortable sticky-header')[3] # Finding the exact table on the webpage I want to get data from\n",
    "noms_col_data4 = noms_tbl4.find_all('tr') # Finding the data to fill the columns within the <tr> tags\n",
    "for row in noms_col_data4:\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    length = len(bestPicture) # Get the number of columns in the dataframe\n",
    "    if not individual_row_data: # If the list of data is empty - indicates header row\n",
    "        continue # Don't add it to the df, just move onto the next row in the Wikitable\n",
    "    else:\n",
    "        bestPicture.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d077b7a-b329-48c7-a569-9cbcf69a4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "noms_tbl5 = soup3.find_all('table', class_ = 'wikitable sortable sticky-header')[4] # Finding the exact table on the webpage I want to get data from\n",
    "noms_col_data5 = noms_tbl5.find_all('tr') # Finding the data to fill the columns within the <tr> tags\n",
    "for row in noms_col_data5:\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    length = len(bestPicture) # Get the number of columns in the dataframe\n",
    "    if not individual_row_data: # If the list of data is empty - indicates header row\n",
    "        continue # Don't add it to the df, just move onto the next row in the Wikitable\n",
    "    else:\n",
    "        bestPicture.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224c8a0-d463-46f6-bbb5-a4b983830298",
   "metadata": {},
   "outputs": [],
   "source": [
    "noms_tbl6 = soup3.find_all('table', class_ = 'wikitable sortable sticky-header')[5] # Finding the exact table on the webpage I want to get data from\n",
    "noms_col_data6 = noms_tbl6.find_all('tr') # Finding the data to fill the columns within the <tr> tags\n",
    "for row in noms_col_data6:\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    length = len(bestPicture) # Get the number of columns in the dataframe\n",
    "    if not individual_row_data: # If the list of data is empty - indicates header row\n",
    "        continue # Don't add it to the df, just move onto the next row in the Wikitable\n",
    "    else:\n",
    "        bestPicture.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b1056-5252-4bf1-bc03-7d7be7ee38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "noms_tbl7 = soup3.find_all('table', class_ = 'wikitable sortable sticky-header')[6] # Finding the exact table on the webpage I want to get data from\n",
    "noms_col_data7 = noms_tbl7.find_all('tr') # Finding the data to fill the columns within the <tr> tags\n",
    "for row in noms_col_data7:\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    length = len(bestPicture) # Get the number of columns in the dataframe\n",
    "    if not individual_row_data: # If the list of data is empty - indicates header row\n",
    "        continue # Don't add it to the df, just move onto the next row in the Wikitable\n",
    "    else:\n",
    "        bestPicture.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81db14-78a2-4b6e-bf2d-3fa6e0884297",
   "metadata": {},
   "outputs": [],
   "source": [
    "noms_tbl8 = soup3.find_all('table', class_ = 'wikitable sortable sticky-header')[7] # Finding the exact table on the webpage I want to get data from\n",
    "noms_col_data8 = noms_tbl8.find_all('tr') # Finding the data to fill the columns within the <tr> tags\n",
    "for row in noms_col_data8:\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    length = len(bestPicture) # Get the number of columns in the dataframe\n",
    "    if not individual_row_data: # If the list of data is empty - indicates header row\n",
    "        continue # Don't add it to the df, just move onto the next row in the Wikitable\n",
    "    else:\n",
    "        bestPicture.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2496c0-5f07-4af3-98b2-14a8d9e25904",
   "metadata": {},
   "outputs": [],
   "source": [
    "noms_tbl9 = soup3.find_all('table', class_ = 'wikitable sortable sticky-header')[8] # Finding the exact table on the webpage I want to get data from\n",
    "noms_col_data9 = noms_tbl9.find_all('tr') # Finding the data to fill the columns within the <tr> tags\n",
    "for row in noms_col_data9: \n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    length = len(bestPicture) # Get the number of columns in the dataframe\n",
    "    if not individual_row_data: # If the list of data is empty - indicates header row\n",
    "        continue # Don't add it to the df, just move onto the next row in the Wikitable\n",
    "    else:\n",
    "        bestPicture.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3430b-6a19-466e-a5c6-f699c7f099fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "noms_tbl10 = soup3.find_all('table', class_ = 'wikitable sortable sticky-header')[9] # Finding the exact table on the webpage I want to get data from\n",
    "noms_col_data10 = noms_tbl10.find_all('tr') # Finding the data to fill the columns within the <tr> tags\n",
    "for row in noms_col_data10:\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    length = len(bestPicture) # Get the number of columns in the dataframe\n",
    "    if not individual_row_data: # If the list of data is empty - indicates header row\n",
    "        continue # Don't add it to the df, just move onto the next row in the Wikitable\n",
    "    else:\n",
    "        bestPicture.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad1180-177d-4cbf-b55d-8297c92c1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "noms_tbl11 = soup3.find_all('table', class_ = 'wikitable sortable sticky-header')[10] # Finding the exact table on the webpage I want to get data from\n",
    "noms_col_data11 = noms_tbl11.find_all('tr') # Finding the data to fill the columns within the <tr> tags\n",
    "for row in noms_col_data11:\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    length = len(bestPicture) # Get the number of columns in the dataframe\n",
    "    if not individual_row_data:# If the list of data is empty - indicates header row\n",
    "        continue # Don't add it to the df, just move onto the next row in the Wikitable\n",
    "    else:\n",
    "        bestPicture.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a42ed-2447-4c1b-8151-79d20eda3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a table w/ all the nominees for academy award for best picture\n",
    "bestPicture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f0b23-2e53-480b-9071-f83131866566",
   "metadata": {},
   "source": [
    "## Webpage 4: Highest Paid Actors and Actresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b72d0-5d83-4dfc-93df-a925ebadcd97",
   "metadata": {},
   "source": [
    "### Step 1: Setting up a web scraping pipeline to collect all the data from the Wikitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff817f82-725c-49c8-aebd-b2b35f6d820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the text from the webpage and placing it in a soup object\n",
    "url4 = 'https://en.wikipedia.org/wiki/List_of_highest-paid_film_actors#'\n",
    "page4 = requests.get(url4)\n",
    "soup4 = BeautifulSoup(page4.text, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de99fc9-9285-422f-9768-ee8347232174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the exact table on the webpage I want to get data from\n",
    "table4 = soup4.find_all('table', class_ = 'wikitable sortable plainrowheaders')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e048387-654f-4503-8948-edc5094b9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all the titles for the table in the <th> tags\n",
    "titles4 = table4.find_all('th')\n",
    "table4_titles = [title.text.strip() for title in titles4]\n",
    "table4_titles = table4_titles[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480fb36a-f067-4b15-91dd-fc0852f1d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table4_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39242a03-d2ee-40ba-814c-5f724831b6f9",
   "metadata": {},
   "source": [
    "### Step 2: Creating a Pandas dataframe and placing the scraped data into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9752ec-939a-490a-a0b9-af426d92591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame with the table titles as column names\n",
    "highestPaid = pd.DataFrame(columns = table4_titles)\n",
    "highestPaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b04a1-a1cd-4f13-a7be-ccf5e602b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unecessary columns\n",
    "highestPaid.drop('Year', axis=1, inplace=True)\n",
    "highestPaid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aaebdc-ccef-47c6-a0b6-bedce9105ceb",
   "metadata": {},
   "source": [
    "#### Note: I dropped the year column because it was unecessary for my analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55beca6-24ce-4c04-948b-853798854cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the data to fill the columns within the <tr> tags\n",
    "tbl4_col_data = table4.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18892248-e444-4281-ada5-d36f52ff0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tbl4_col_data[1:]: # Start at the second row to ignore the header row\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    length = len(highestPaid) # Get the number of columns in the dataframe\n",
    "    highestPaid.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6040c1-2187-42cc-b11d-ad1e433189b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "highestPaid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7733241-069d-4ca1-b4c9-e005357ea744",
   "metadata": {},
   "source": [
    "### Step 3: Performing additional cleaning operations on the dataframe to make it useful for answering my question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d65a8-595d-4dbb-9785-2599dd5f9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unecessary columns\n",
    "highestPaid.drop('Earnings', axis=1, inplace=True)\n",
    "highestPaid.drop('Ref.', axis=1, inplace=True)\n",
    "highestPaid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d285127-d103-4c01-91c5-340b20bd34b4",
   "metadata": {},
   "source": [
    "#### Note: I dropped these columns because they were unecessary for my data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095802bf-d15f-4244-a2fa-65a0152945ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting my big dataframe at the second columm to create a seperate frame for actor data\n",
    "highestPaidActors = highestPaid.iloc[:, :1]\n",
    "highestPaidActors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa892a58-f8dd-4680-9b21-200457b5313e",
   "metadata": {},
   "source": [
    "#### Note: I learned how to split my dataframe into two from a suggestion on Stack Overflow: https://stackoverflow.com/questions/41624241/pandas-split-dataframe-into-two-dataframes-at-a-specific-column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40cd1c8-d055-48cd-8c05-ee7d9cbb9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting my big dataframe at the second columm to create a seperate frame for actress data\n",
    "highestPaidActresses = highestPaid.iloc[:, 1:]\n",
    "highestPaidActresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c517ba5e-64db-4202-853a-4059f3fdb386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping multiple instances of a name to only ensure each name occured once\n",
    "highestPaidActors = highestPaidActors.drop_duplicates(subset=['Actor'], keep='first')\n",
    "highestPaidActors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc536830-c0db-4716-aab8-15c6c3d87333",
   "metadata": {},
   "source": [
    "#### Note: I used this GeeksForGeeks tutorial to help me with the drop duplicates method: https://www.geeksforgeeks.org/pandas/python-pandas-dataframe-drop_duplicates/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb50b50b-573d-4e5d-9859-336716375c3e",
   "metadata": {},
   "source": [
    "#### Note: The Wikitable this originally came from had listed the highest paid actor/actress for each year. Because an actor could be the highest paid actor 2+ years in a row, I removed duplicates so that only one instance of the actor remained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b218b70b-3fe0-4eb1-9daf-8d916e336087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping multiple instances of a name to only ensure each name occured once\n",
    "highestPaidActresses = highestPaidActresses.drop_duplicates(subset=['Actress'], keep='first')\n",
    "highestPaidActresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f1c6c2-c863-42e2-a3a5-98b8240c5453",
   "metadata": {},
   "source": [
    "## Webpage 5: Nominees for Academy Award for Best Actress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5fa097-8861-4f8c-8c14-52bcc62fffbb",
   "metadata": {},
   "source": [
    "### Step 1: Setting up a web scraping pipeline to collect all the data from the Wikitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48203dc-33fe-4d2a-9a97-4ef470e3a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the text from the webpage and placing it in a soup object\n",
    "url5 = 'https://en.wikipedia.org/wiki/Academy_Award_for_Best_Actress#'\n",
    "page5 = requests.get(url5)\n",
    "soup5 = BeautifulSoup(page5.text, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea1f2c-3e0f-4ea3-8724-fb50c992389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc59a9-da10-4bc3-9602-659c913831f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the exact table on the webpage I want to get data from\n",
    "table5 = soup5.find_all('table', class_ = 'wikitable sortable')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b9dffe-9e8d-4b73-bdb0-f794a4eca6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all the titles for the table in the <th> tags\n",
    "titles5 = table5.find_all('th')\n",
    "table5_titles = [title.text.strip() for title in titles5]\n",
    "table5_titles = table5_titles[:5]\n",
    "table5_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977358c-0ed2-4d6c-a8a9-ef3a4b324b0b",
   "metadata": {},
   "source": [
    "### Step 2: Creating a Pandas dataframe and placing the scraped data into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed5e1e7-f366-42a9-abde-dbde84352b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame with the table titles as column names\n",
    "bestActresses = pd.DataFrame(columns = table5_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28641c-2298-4962-af7f-9c54c68d8b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestActresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4acfd8-0fea-430f-9c64-c69606b4f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unecessary columns\n",
    "bestActresses.drop('Year', axis=1, inplace=True)\n",
    "bestActresses.drop('Role(s)', axis=1, inplace=True)\n",
    "bestActresses.drop('Film', axis=1, inplace=True)\n",
    "bestActresses.drop('Ref.', axis=1, inplace=True)\n",
    "bestActresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d913a842-6707-4855-b75f-040e3e5c2046",
   "metadata": {},
   "source": [
    "#### Note: I just wanted this to be a list of actresses who were nominees/winners, so I got rid of all the columns except the actress names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe238228-1bc0-4329-81a3-76737c7617bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "actress_tbl7 = soup5.find_all('table', class_ = 'wikitable sortable')[6] # Finding the exact table on the webpage I want to get data from\n",
    "actress_col_data7 = actress_tbl7.find_all('tr') # Finding the data to fill the columns within the <tr> tags\n",
    "for row in actress_col_data7[36:]: #Choosing which row in the table I want to start from (see note)\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    individual_row_data = individual_row_data[:1] #Slice the data to only contain the text w/ the name of the actress, because that's all thats needed\n",
    "\n",
    "    length = len(bestActresses) # Get the number of columns in the dataframe\n",
    "    if not individual_row_data: # If the list of data is empty - indicates header row\n",
    "        continue # Don't add it to the df, just move onto the next row in the Wikitable\n",
    "    else:\n",
    "        bestActresses.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0bfc12-3769-4888-ba89-fd6edc49d50e",
   "metadata": {},
   "source": [
    "#### Note: I start from Row 36 in this table because that's where the year 1987 is on the Wikitable. The list this would be compared to only contains data from 1987 onwards, so I wanted to only have actress nominations from 1987 onwards in my df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e0fa4b-2cef-41e3-8c33-7fd3ca6d55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "actress_tbl8 = soup5.find_all('table', class_ = 'wikitable sortable')[7]\n",
    "actress_col_data8 = actress_tbl8.find_all('tr')\n",
    "for row in actress_col_data8:\n",
    "    row_data = row.find_all('td')\n",
    "    individual_row_data = [data.text.strip() for data in row_data]\n",
    "    individual_row_data = individual_row_data[:1]\n",
    "\n",
    "    length = len(bestActresses)\n",
    "    if not individual_row_data:\n",
    "        continue\n",
    "    else:\n",
    "        bestActresses.loc[length] = individual_row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549bcbea-66f7-4fe6-b99c-83a6d8d2c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "actress_tbl9 = soup5.find_all('table', class_ = 'wikitable sortable')[8]\n",
    "actress_col_data9 = actress_tbl9.find_all('tr')\n",
    "for row in actress_col_data9:\n",
    "    row_data = row.find_all('td')\n",
    "    individual_row_data = [data.text.strip() for data in row_data]\n",
    "    individual_row_data = individual_row_data[:1]\n",
    "\n",
    "    length = len(bestActresses)\n",
    "    if not individual_row_data:\n",
    "        continue\n",
    "    else:\n",
    "        bestActresses.loc[length] = individual_row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa9a94-8c1e-4efa-ad27-e37f7fe282a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "actress_tbl10 = soup5.find_all('table', class_ = 'wikitable sortable')[9]\n",
    "actress_col_data10 = actress_tbl10.find_all('tr')\n",
    "for row in actress_col_data10:\n",
    "    row_data = row.find_all('td')\n",
    "    individual_row_data = [data.text.strip() for data in row_data]\n",
    "    individual_row_data = individual_row_data[:1]\n",
    "\n",
    "    length = len(bestActresses)\n",
    "    if not individual_row_data:\n",
    "        continue\n",
    "    else:\n",
    "        bestActresses.loc[length] = individual_row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c7153-a52d-406a-a480-0593c5afc5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "actress_tbl11 = soup5.find_all('table', class_ = 'wikitable sortable')[10]\n",
    "actress_col_data11 = actress_tbl11.find_all('tr')\n",
    "for row in actress_col_data11:\n",
    "    row_data = row.find_all('td')\n",
    "    individual_row_data = [data.text.strip() for data in row_data]\n",
    "    individual_row_data = individual_row_data[:1]\n",
    "\n",
    "    length = len(bestActresses)\n",
    "    if not individual_row_data:\n",
    "        continue\n",
    "    else:\n",
    "        bestActresses.loc[length] = individual_row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478668aa-2835-461f-9895-438aba756dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestActresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89448830-a3b7-4df8-98b2-331630765642",
   "metadata": {},
   "source": [
    "## Webpage 6: Nominees for Academy Award for Best Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a15a6-0141-4c30-b8bd-93cdda52468c",
   "metadata": {},
   "source": [
    "### Step 1: Setting up a web scraping pipeline to collect all the data from the Wikitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6bf9b7-bbeb-46b7-97c1-ba26abdb8435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the text from the webpage and placing it in a soup object\n",
    "url6 = 'https://en.wikipedia.org/wiki/Academy_Award_for_Best_Actor#'\n",
    "page6 = requests.get(url6)\n",
    "soup6 = BeautifulSoup(page6.text, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cce1eca-e095-4974-9f00-c3385e44dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f008d351-dd19-43c9-92da-bc81109154a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the exact table on the webpage I want to get data from\n",
    "table6 = soup6.find_all('table', class_ = 'wikitable sortable')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c699950-1f50-4581-80bc-1f672bb502a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ce839-6a9a-49f0-a212-b74a0cd9acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all the titles for the table in the <th> tags\n",
    "titles6 = table6.find_all('th')\n",
    "table6_titles = [title.text.strip() for title in titles6]\n",
    "table6_titles = table6_titles[:5]\n",
    "table6_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb1e7c-5bab-4206-abb5-32c09de1550b",
   "metadata": {},
   "source": [
    "### Step 2: Creating a Pandas dataframe to place all the scraped data into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab95a53-b75c-4a00-9b54-c691409d4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame with the table titles as column names\n",
    "bestActors = pd.DataFrame(columns = table6_titles)\n",
    "bestActors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09de7f-fa89-445e-a81d-f7ed517b992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unecessary columns\n",
    "bestActors.drop('Year', axis=1, inplace=True)\n",
    "bestActors.drop('Role(s)', axis=1, inplace=True)\n",
    "bestActors.drop('Film', axis=1, inplace=True)\n",
    "bestActors.drop('Ref.', axis=1, inplace=True)\n",
    "bestActors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d930575-aa41-4733-973d-b1aa38a26c78",
   "metadata": {},
   "source": [
    "#### Note: I only really wanted the list of actor names to work with, so I dropped all the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031b46b-3642-433f-8c03-8b609c156179",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_tbl7 = soup6.find_all('table', class_ = 'wikitable sortable')[6] # Finding the exact table on the webpage I want to get data from\n",
    "actor_col_data7 = actor_tbl7.find_all('tr') # Finding the data to fill the columns within the <tr> tags\n",
    "for row in actor_col_data7[36:]: #Choosing which row in the table I want to start from (see note)\n",
    "    row_data = row.find_all('td') # Find all the data within a row in the <td> tags\n",
    "    individual_row_data = [data.text.strip() for data in row_data] # Strip any tags/whitespace characters from row data, place each row in a list\n",
    "    individual_row_data = individual_row_data[:1] #Slice the data to only contain the text w/ the name of the actor, because that's all thats needed\n",
    "\n",
    "    length = len(bestActors) # Get the number of columns in the dataframe\n",
    "    if not individual_row_data: # If the list of data is empty - indicates header row\n",
    "        continue # Don't add it to the df, just move onto the next row in the Wikitable\n",
    "    else:\n",
    "        bestActors.loc[length] = individual_row_data # If the number of elements in the list matches the number of columns in the df, add the list in as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31335c2-1782-43b0-b047-fd58f2a74407",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_tbl8 = soup6.find_all('table', class_ = 'wikitable sortable')[7]\n",
    "actor_col_data8 = actor_tbl8.find_all('tr')\n",
    "for row in actor_col_data8:\n",
    "    row_data = row.find_all('td')\n",
    "    individual_row_data = [data.text.strip() for data in row_data]\n",
    "    individual_row_data = individual_row_data[:1]\n",
    "\n",
    "    length = len(bestActors)\n",
    "    if not individual_row_data:\n",
    "        continue\n",
    "    else:\n",
    "        bestActors.loc[length] = individual_row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741502d0-b2bc-4361-bf3e-2116779a4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_tbl9 = soup6.find_all('table', class_ = 'wikitable sortable')[8]\n",
    "actor_col_data9 = actor_tbl9.find_all('tr')\n",
    "for row in actor_col_data9:\n",
    "    row_data = row.find_all('td')\n",
    "    individual_row_data = [data.text.strip() for data in row_data]\n",
    "    individual_row_data = individual_row_data[:1]\n",
    "\n",
    "    length = len(bestActors)\n",
    "    if not individual_row_data:\n",
    "        continue\n",
    "    else:\n",
    "        bestActors.loc[length] = individual_row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060d007-a529-4d71-912e-2e144091d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_tbl10 = soup6.find_all('table', class_ = 'wikitable sortable')[9]\n",
    "actor_col_data10 = actor_tbl10.find_all('tr')\n",
    "for row in actor_col_data10:\n",
    "    row_data = row.find_all('td')\n",
    "    individual_row_data = [data.text.strip() for data in row_data]\n",
    "    individual_row_data = individual_row_data[:1]\n",
    "\n",
    "    length = len(bestActors)\n",
    "    if not individual_row_data:\n",
    "        continue\n",
    "    else:\n",
    "        bestActors.loc[length] = individual_row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee903a3-af79-414d-b7fc-bd6dc6319d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_tbl11 = soup6.find_all('table', class_ = 'wikitable sortable')[10]\n",
    "actor_col_data11 = actor_tbl11.find_all('tr')\n",
    "for row in actor_col_data11:\n",
    "    row_data = row.find_all('td')\n",
    "    individual_row_data = [data.text.strip() for data in row_data]\n",
    "    individual_row_data = individual_row_data[:1]\n",
    "\n",
    "    length = len(bestActors)\n",
    "    if not individual_row_data:\n",
    "        continue\n",
    "    else:\n",
    "        bestActors.loc[length] = individual_row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91642967-2b53-47fd-bd77-5646f8682579",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestActors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bddbbd-4077-4c7f-b408-c8aa5e686a05",
   "metadata": {},
   "source": [
    "# Part 3: Plotting\n",
    "## In this section, I used Matplotlib and Seaborn to make visualizations that serve as answers to my questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c6cce-68f9-41b4-8d7e-b345f147691e",
   "metadata": {},
   "source": [
    "## Question 1: Which decade had the most top-earning movies (adjusted for inflation)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c0fc1-7989-4720-b148-f6908c3bf91d",
   "metadata": {},
   "source": [
    "### Step 1: Manipulating the data within the dataframe to come to a conclusion to my answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ea838-6d64-4c14-938e-4eb074d25df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by year in ascending order to determine the range of decades I'll need to include in my dictionary\n",
    "sorted_df = highestGrossing.sort_values(by='Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fe3664-a787-4633-a92e-270f602c0511",
   "metadata": {},
   "source": [
    "#### Note: I used this GeeksForGeeks tutorial to help me sort the data by the year to see the range of decades I'd need: https://www.geeksforgeeks.org/pandas/how-to-sort-pandas-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b371fe-01c3-4210-9069-4dbf3f3eb3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the sorted dataframe to determine what the earliest and latest decade are\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41eec65-0d15-4381-8995-af2415d0e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping a decade to the number of top grossing movies in that decade\n",
    "decadesCt = {1930:0, 1940:0, 1950:0, 1960:0, 1970:0, 1980:0, 1990:0, 2000:0, 2010:0, 2020:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e70e58-ba63-4442-854a-6d712d14bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list only containing the key values of the dictionary to use to parse through the dictionary\n",
    "dictKeys = list(decadesCt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc48c95-ff3d-4c38-8661-1a413536fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each movie, determine what decade that movie's release date falls in, then increment the value corresponding to the key that decade is in the dict\n",
    "for yr in highestGrossing['Year']:\n",
    "    if (int(yr) >= dictKeys[0]) and (int(yr) < dictKeys[1]):\n",
    "        decadesCt[1930] += 1\n",
    "    elif (int(yr) >= dictKeys[1]) and (int(yr) < dictKeys[2]):\n",
    "        decadesCt[1940] += 1\n",
    "    elif (int(yr) >= dictKeys[2]) and (int(yr) < dictKeys[3]):\n",
    "        decadesCt[1950] += 1\n",
    "    elif (int(yr) >= dictKeys[3]) and (int(yr) < dictKeys[4]):\n",
    "        decadesCt[1960] += 1\n",
    "    elif (int(yr) >= dictKeys[4]) and (int(yr) < dictKeys[5]):\n",
    "        decadesCt[1970] += 1\n",
    "    elif (int(yr) >= dictKeys[5]) and (int(yr) < dictKeys[6]):\n",
    "        decadesCt[1980] += 1\n",
    "    elif (int(yr) >= dictKeys[6]) and (int(yr) < dictKeys[7]):\n",
    "        decadesCt[1990] += 1\n",
    "    elif (int(yr) >= dictKeys[7]) and (int(yr) < dictKeys[8]):\n",
    "        decadesCt[2000] += 1\n",
    "    elif (int(yr) >= dictKeys[8]) and (int(yr) < dictKeys[9]):\n",
    "        decadesCt[2010] += 1\n",
    "    elif (int(yr) >= dictKeys[9]):\n",
    "        decadesCt[2020] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fc47f-e5dd-40bb-a8f0-f4b8eb6d3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the number of top grossing movies in the decade\n",
    "decadesCt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11745ea-402d-49dc-bc59-8d5601468f1e",
   "metadata": {},
   "source": [
    "### Step 2: Creating a plot based on my numerical conclusions to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f63bb62-a05a-4180-8af6-307b4730b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart that illustrates the data in the dictionary\n",
    "sns.barplot(x=list(decadesCt.keys()), y=list(decadesCt.values()), palette='Set2')\n",
    "sns.set_style('whitegrid')\n",
    "plt.title('The 1970s Had the Most Top-Grossing Movies of All Time')\n",
    "plt.xlabel('Decade')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.ylim(0, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776dc7fa-e383-43dc-8050-925438122372",
   "metadata": {},
   "source": [
    "#### Note: I used the Seaborn docs to help me make my bar chart: https://seaborn.pydata.org/generated/seaborn.barplot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8cc8b9-7dfd-4637-8453-e28677aa79bf",
   "metadata": {},
   "source": [
    "### Analysis: As the title of the graph states, the most top-grossing movies were released in the 1970s. I would have expected more of the top grossing movies to be released in more recent decades (2000s onward), as movie tickets tend to be more expensive nowadays, but since the data for this graph was inflation-adjusted it makes sense that it would account for price changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77016d6-abb9-4c1c-8759-80b164caab3e",
   "metadata": {},
   "source": [
    "## Question 2: Do the highest-grossing movies have a better likelihood of being nominated for the Academy Award for Best Picture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d947545-9d5d-430d-a557-21d5f5d22759",
   "metadata": {},
   "source": [
    "### Step 1: Manipulating the data within the dataframe to come to a conclusion to my answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c07c6-25c1-490d-ab58-4e6bd1e7b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to compare the number of top-grossing films nominated for best picture vs those not nominated\n",
    "nomineeCts = {'Nominated':0, 'Not Nominated':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2257dcd-4255-4d16-b116-12e005d38e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists of the nominees and highest grossing films\n",
    "nomineeNames = bestPicture['Film'].tolist()\n",
    "highestGrossNames = highestGrossing['Title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810449b1-e63f-4e37-929d-584376c00b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using set comprehensions to find the highest grossing films in the list of nominees\n",
    "matches = list(set(nomineeNames).intersection(set(highestGrossNames)))\n",
    "# Setting the dictionary values equal to the number of films that are in both films and the number that are not, respectively\n",
    "nomineeCts['Nominated'] = len(matches)\n",
    "nomineeCts['Not Nominated'] = (len(nomineeNames) - len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca70b0b-03f6-4441-8587-503447cc2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the number of films nominated vs. not nominated\n",
    "nomineeCts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a64bb0-76a2-43dd-9254-dfb48363c4dd",
   "metadata": {},
   "source": [
    "### Step 2: Creating a plot based on my numerical conclusions to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f87e7-32c8-4da6-885e-524b43a09d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pie chart to illustrate the percent of high grossing films that get nominated for Best Picture\n",
    "labels = list(nomineeCts.keys())\n",
    "sizes = list(nomineeCts.values())\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Top Grossing Movies Do Not Have a Better Likelihood of Being Nominated for Best Picture')\n",
    "plt.axis('equal') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a2727f-b693-41fd-90cb-055513402943",
   "metadata": {},
   "source": [
    "#### I used this tutorial to help me make my pie chart: https://pieriantraining.com/seaborn-pie-chart-a-tutorial-for-data-visualization/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1f54a-89ec-4814-8724-ced44c7ca91b",
   "metadata": {},
   "source": [
    "### Analysis: I had assumed that there would be more films nominated for Best Picture in the highest grossing films list, as recieving the Best Picture award usually gives films some notoriety which encourages many people to go watch them. However, it also makes sense that many of the highest grossing films aren't Best Picture nominees, because the Academy Awards usually go to more artistic films, and the highest grossers tend to be more commercial type films"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b48b0-44fb-4ec6-b0e9-bfa7a155de74",
   "metadata": {},
   "source": [
    "## Question 3a: Do the highest-paid actors get nominated for more Academy Awards for Best Actor?: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f84226-079d-4a2d-9580-00c06a25dc6c",
   "metadata": {},
   "source": [
    "### Step 1: Manipulating the data within the dataframe to come to a conclusion to my answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e66f8a-12ff-45a0-8e88-dbaa649970df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting all the data within the actors column into a list to perform comparisons on\n",
    "actors = list(highestPaidActors['Actor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d10e6-a89b-454b-8d51-c2fe8837ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary mapping each high-paid actor to the number of nominations they recieve\n",
    "actor_correlation = dict.fromkeys(actors, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e496f-4199-4ff0-bcd4-482c8ad03efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing each actor name to the list of nominees, and incrementing the value each time an actor's name is found\n",
    "for key in actor_correlation.keys():\n",
    "    for val in list(bestActors['Actor']):\n",
    "        if (key == val) or (key+\" ‡\" == val): # See note below\n",
    "            actor_correlation[key] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d5e51-56f1-4c93-b5d2-f5c3c3668b7a",
   "metadata": {},
   "source": [
    "#### Note: The Wikipedia list used special characters to denote winners of the award. Because winners are also nominees, I made sure to include the second statement to ensure that all instances of an actor's name get recorded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933cfd42-472c-41db-99df-a50aaa6ca673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the dictionary with each actor and their number of nominees\n",
    "actor_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460bfac9-d1dc-49ec-9c8e-a881a6a9e785",
   "metadata": {},
   "source": [
    "### Step 2: Creating a plot based on my numerical conclusions to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4582e47-c85c-4740-a4f1-27a06ae930f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a histogram to illustrate the likelihood of being a top-paying actor and getting nominated for the Best Actor award\n",
    "actor_nomination_counts = list(actor_correlation.values())\n",
    "sns.histplot(actor_nomination_counts, bins=range(0, max(actor_nomination_counts) + 2), kde=False)\n",
    "plt.xlabel('Number of Nominations')\n",
    "plt.ylabel('Number of Actors')\n",
    "plt.title('The Highest Paid Actors Do Not Get Nominated for More Academy Awards')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6a746-d5b1-4a6a-b064-341339868806",
   "metadata": {},
   "source": [
    "#### I was actually unsure of how I could represent the results in the dictionary visually, so I asked ChatGPT and it recommended I use a histogram\n",
    "#### I used the Seaborn docs to help me with the creation of the histogram: https://seaborn.pydata.org/generated/seaborn.histplot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4102ab-1b14-46f9-869f-b7a9757f387c",
   "metadata": {},
   "source": [
    "## Question 3b: Do the highest-paid actresses get nominated for more Academy Awards for Best Actress?: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398c731-fffd-479b-bd5b-354f399b676f",
   "metadata": {},
   "source": [
    "### Step 1: Manipulating the data within the dataframe to come to a conclusion to my answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e010bb5-8c2c-4670-b1dc-24517fa556fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting all the data within the actresses column into a list to perform comparisons on\n",
    "actresses = list(highestPaidActresses['Actress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d916b1-6d1a-4ac8-b8e6-85cc8e1bc5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary mapping each high-paid actress to the number of nominations they recieve\n",
    "actress_correlation = dict.fromkeys(actresses, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a065a-7563-4b6e-9bbf-9aefc66d38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing each actor name to the list of nominees, and incrementing the value each time an actress's name is found\n",
    "for key in actress_correlation.keys():\n",
    "    for val in list(bestActresses['Actress']):\n",
    "        if (key == val) or (key+\" ‡\" == val):\n",
    "            actress_correlation[key] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a026a7d6-acfb-4c31-a1af-22aefac5dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the dictionary with each actress and their number of nominees\n",
    "actress_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc3bed-0555-475e-8464-167ba6b8c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unecessary values from the dictionary\n",
    "actress_correlation.pop('—')\n",
    "actress_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615174e-1168-4409-8071-f5cedd39b6fb",
   "metadata": {},
   "source": [
    "#### Note: The original table on Wikipedia actually included the dash, so it got scraped in. I dropped it from my data because it was irrelevant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94c7d1-85a0-42f1-b21f-d5cd15af2a67",
   "metadata": {},
   "source": [
    "### Step 2: Creating a plot based on my numerical conclusions to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a9ed5-f36f-4a3b-86d2-9a7bbb752dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a histogram to illustrate the likelihood of being a top-paying actress and getting nominated for the Best Actor award\n",
    "actress_nomination_counts = list(actress_correlation.values())\n",
    "\n",
    "sns.histplot(actress_nomination_counts, bins=range(0, max(actress_nomination_counts) + 2), kde=False)\n",
    "\n",
    "plt.xlabel('Number of Nominations')\n",
    "plt.ylabel('Number of Actresses')\n",
    "plt.title('The Highest Paid Actresses Do Not Get Nominated for More Academy Awards')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536ab9f-3f77-46fd-b45d-c98acce8634d",
   "metadata": {},
   "source": [
    "### Analysis: I was suprised that the fact that higher-paid actors do not get nominated for more awards, because I assumed that people were willing to pay them more because they delivered better performances. Then I reviewed the Wikitable again and realized that it contained data on the highest paid actors across all media types (film, theater, TV, etc.), while the Academy Awards are only awarded to film actors, which may have skewed the data a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f6fda6-2bb1-4052-a5a3-99b67bd0e4a7",
   "metadata": {},
   "source": [
    "## Question 4: Do more expensive films make more money at the box office?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b6932-19fd-4eea-a340-70ccdcb8dd6d",
   "metadata": {},
   "source": [
    "### Step 1: Manipulating the data within the dataframe to come to a conclusion to my answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e606c-d95b-42b3-a55c-5b52434455e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two data frames on a common column to have Cost and Earning data in the same df\n",
    "merged = pd.merge(mostExpensive, highestGrossing, on='Title', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c6922-e3da-45f4-81d4-658cfba0eca3",
   "metadata": {},
   "source": [
    "#### Note: I used this GeeksForGeeks tutorial to help me merge my two dataframes into one: https://www.geeksforgeeks.org/pandas/how-to-combine-two-dataframe-in-python-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aeaa23-7a79-43cb-b1a2-134b93c13d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the merged data frame to ensure everything was successful\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dcf3c7-c4ec-450d-b640-529448c6b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove $, commas, asterisks, etc.\n",
    "merged['Cost (est.)(millions)'] = merged['Cost (est.)(millions)'].str.replace('$', '')\n",
    "merged['Cost (est.)(millions)'] = merged['Cost (est.)(millions)'].str.replace('*', '')\n",
    "merged['Cost (est.)(millions)'] = merged['Cost (est.)(millions)'].astype(int)\n",
    "merged['Adjusted gross'] = merged['Adjusted gross'].str.replace('$', '')\n",
    "merged['Adjusted gross'] = merged['Adjusted gross'].str.replace('*', '')\n",
    "merged['Adjusted gross'] = merged['Adjusted gross'].str.replace(',', '')\n",
    "merged['Adjusted gross'] = merged['Adjusted gross'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4ceace-af43-41c0-b2e5-971db4af8db6",
   "metadata": {},
   "source": [
    "#### Note: I used this StackOverflow thread to help me strip the columns of all their nonumerical data + turn them into numbers: https://stackoverflow.com/questions/38516481/trying-to-remove-commas-and-dollars-signs-with-pandas-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa08ff-98c8-42fd-bd89-54be0c969b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the stripped dataframe to ensure that only the numerical values remain\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1780f9e5-582b-471f-9a0a-c0dc1299e56f",
   "metadata": {},
   "source": [
    "### Step 2: Creating a plot based on my numerical conclusions to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf15c1a-8501-4866-bdd7-548c918bb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot w/ regression line to illustrate the relationship between cost and earnings\n",
    "sns.regplot(data=merged, x=\"Cost (est.)(millions)\", y=\"Adjusted gross\", scatter_kws={'s':25}, color='green', marker='s')\n",
    "\n",
    "plt.title('More Expensive Films Do Not Typically Make More Money')\n",
    "plt.xlabel('Cost (in millions)')\n",
    "plt.ylabel('Earnings (in billions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bad7fb-4eb4-462f-8dc3-a4df0862466c",
   "metadata": {},
   "source": [
    "#### Note: I used the Seaborn docs to help me make my scatterplot w/ regression line: https://seaborn.pydata.org/generated/seaborn.regplot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e4b69-f90a-4e23-9f7c-92d047af39da",
   "metadata": {},
   "source": [
    "### Analysis: There is little to no correlation between how much money is spent on a film and the amount it earns in the box office. I was surprised to learn this, as I assumed that films that spent a lot of money would have put that money towards making the film as good as possible, so it could earn all the money back. However, it is interesting to note how there can be films that spend almost the same amount of money but make drastically different amounts in revenue, such as the two data points in the 400/450 mil budget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a7b01-ab28-4bd0-97d4-4461c517b10f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
